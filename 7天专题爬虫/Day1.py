'''第一天'''
'''
1.什么是爬虫，为什么要用爬虫？
答：用于在网络上采集数据的程序，可以用任何语言开发，python更加方便快捷高效一些。
爬虫的目的：采集一些需要的数据。
python中封装了很多爬虫库，如urllib ,re,bs,scrapy等，开发效率更高
'''
'''
2.简述seleninu模块的作用及基本使用？
答：selenium最初是一个自动化测试工具,而爬虫中使用它主要是为了解决requests无法直接执行JavaScript代码的问题
selenium本质是通过驱动浏览器，完全模拟浏览器的操作，比如跳转、输入、点击、下拉等，来拿到网页渲染之后的结果，可支持多种浏览器
'''
'''
3.HTTPS是如何实现安全传输数据的？
答：
1.客户端发起一个http请求，连接到服务器的443端口。
2.服务端把自己的信息以数字证书的形式返回给客户端（证书内容有密钥公钥，网站地址，证书颁发机构，失效日期等）。证书中有一个公钥来加密信息，私钥由服务器持有。
3.验证证书的合法性,客户端收到服务器的响应后会先验证证书的合法性（证书中包含的地址与正在访问的地址是否一致，证书是否过期）。
4.生成随机密码（RSA签名）,如果验证通过，或用户接受了不受信任的证书，浏览器就会生成一个随机的对称密钥（session key）并用公钥加密，让服务端用私钥解密，解密后就用这个对称密钥进行传输了，并且能够说明服务端确实是私钥的持有者。
5.生成对称加密算法,验证完服务端身份后，客户端生成一个对称加密的算法和对应密钥，以公钥加密之后发送给服务端。此时被黑客截获也没用，因为只有服务端的私钥才可以对其进行解密。之后客户端与服务端可以用这个对称加密算法来加密和解密通信内容了。
'''
